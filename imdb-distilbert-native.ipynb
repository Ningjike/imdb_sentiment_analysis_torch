{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc707fe6",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-10-17T09:16:32.787787Z",
     "iopub.status.busy": "2025-10-17T09:16:32.787508Z",
     "iopub.status.idle": "2025-10-17T10:21:13.869698Z",
     "shell.execute_reply": "2025-10-17T10:21:13.868748Z"
    },
    "papermill": {
     "duration": 3881.086691,
     "end_time": "2025-10-17T10:21:13.871141",
     "exception": false,
     "start_time": "2025-10-17T09:16:32.784450",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-17 09:16:58.990071: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1760692619.425375      19 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1760692619.531242      19 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 0: 100%|██████████| 1667/1667 [18:38<00:00,  1.49it/s, epoch=0, train loss=0.2808, train acc=0.88, val loss=0.2203, val acc=0.91, time=1118.60]\n",
      "Epoch 1: 100%|██████████| 1667/1667 [18:55<00:00,  1.47it/s, epoch=1, train loss=0.1458, train acc=0.95, val loss=0.2137, val acc=0.92, time=1136.19]\n",
      "Epoch 2: 100%|██████████| 1667/1667 [18:55<00:00,  1.47it/s, epoch=2, train loss=0.0751, train acc=0.98, val loss=0.2743, val acc=0.91, time=1136.07]\n",
      "Predction: 100%|██████████| 1042/1042 [07:01<00:00,  2.47it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "train = pd.read_csv(\"/kaggle/input/corpus-imdb/labeledTrainData.tsv\", header=0,\n",
    "                    delimiter=\"\\t\", quoting=3)\n",
    "test = pd.read_csv(\"/kaggle/input/corpus-imdb/testData.tsv\", header=0,\n",
    "                    delimiter=\"\\t\", quoting=3)\n",
    "\n",
    "\n",
    "class TrainDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels=None):\n",
    "        self.encodings = encodings\n",
    "        if labels:\n",
    "            self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        if self.labels:\n",
    "            item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "class TestDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, num_samples=0):\n",
    "        self.encodings = encodings\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    program = os.path.basename(sys.argv[0])\n",
    "    logger = logging.getLogger(program)\n",
    "\n",
    "    logging.basicConfig(format='%(asctime)s: %(levelname)s: %(message)s')\n",
    "    logging.root.setLevel(level=logging.INFO)\n",
    "    logger.info(r\"running %s\" % ''.join(sys.argv))\n",
    "\n",
    "    train_texts, train_labels, test_texts = [], [], []\n",
    "    for i, review in enumerate(train[\"review\"]):\n",
    "        train_texts.append(review)\n",
    "        train_labels.append(train['sentiment'][i])\n",
    "\n",
    "    for review in test['review']:\n",
    "        test_texts.append(review)\n",
    "\n",
    "    train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=.2)\n",
    "    model_path = \"/kaggle/input/distilbert-base-uncased\"\n",
    "    tokenizer = DistilBertTokenizerFast.from_pretrained(model_path, local_files_only=True)\n",
    "\n",
    "    train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "    val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n",
    "    test_encodings = tokenizer(test_texts, truncation=True, padding=True)\n",
    "\n",
    "    train_dataset = TrainDataset(train_encodings, train_labels)\n",
    "    val_dataset = TrainDataset(val_encodings, val_labels)\n",
    "    test_dataset = TestDataset(test_encodings, num_samples=len(test_texts))\n",
    "\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "    model = DistilBertForSequenceClassification.from_pretrained(\n",
    "        model_path,\n",
    "        num_labels=2,\n",
    "        local_files_only=True\n",
    "    )\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=12, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=24, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=24, shuffle=False)\n",
    "\n",
    "    optim = optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "    for epoch in range(3):\n",
    "        start = time.time()\n",
    "        train_loss, val_losses = 0, 0\n",
    "        train_acc, val_acc = 0, 0\n",
    "        n, m = 0, 0\n",
    "\n",
    "        with tqdm(total=len(train_loader), desc=\"Epoch %d\" % epoch) as pbar:\n",
    "            for batch in train_loader:\n",
    "                n += 1\n",
    "                optim.zero_grad()\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                loss = outputs.loss\n",
    "                loss.backward()\n",
    "                optim.step()\n",
    "                train_acc += accuracy_score(torch.argmax(outputs.logits.cpu().data, dim=1), labels.cpu())\n",
    "                train_loss += loss.cpu()\n",
    "                pbar.set_postfix({'epoch': '%d' % (epoch),\n",
    "                                  'train loss': '%.4f' % (train_loss.data / n),\n",
    "                                  'train acc': '%.2f' % (train_acc / n)\n",
    "                                  })\n",
    "                pbar.update(1)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for batch in val_loader:\n",
    "                    m += 1\n",
    "                    input_ids = batch['input_ids'].to(device)\n",
    "                    attention_mask = batch['attention_mask'].to(device)\n",
    "                    labels = batch['labels'].to(device)\n",
    "                    outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                    val_loss = outputs.loss\n",
    "                    val_acc += accuracy_score(torch.argmax(outputs.logits.cpu().data, dim=1), labels.cpu())\n",
    "                    val_losses += val_loss\n",
    "            end = time.time()\n",
    "            runtime = end - start\n",
    "            pbar.set_postfix({'epoch': '%d' % (epoch),\n",
    "                              'train loss': '%.4f' % (train_loss.data/ n),\n",
    "                              'train acc': '%.2f' % (train_acc / n),\n",
    "                              'val loss': '%.4f' % (val_losses.data / m),\n",
    "                              'val acc': '%.2f' % (val_acc / m),\n",
    "                              'time': '%.2f' % (runtime)})\n",
    "\n",
    "            # print('epoch: %d, train loss: %.4f, train acc: %.2f, val loss: %.4f, val acc: %.2f, time: %.2f' %\n",
    "            #       (epoch, train_loss.data / n, train_acc / n, val_losses.data / m, val_acc / m, runtime))\n",
    "\n",
    "    test_pred = []\n",
    "    with torch.no_grad():\n",
    "        with tqdm(total=len(test_loader), desc='Predction') as pbar:\n",
    "            for batch in test_loader:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                outputs = model(input_ids, attention_mask=attention_mask)\n",
    "                # test_pred.extent\n",
    "                test_pred.extend(torch.argmax(outputs.logits.cpu().data, dim=1).numpy().tolist())\n",
    "\n",
    "                pbar.update(1)\n",
    "\n",
    "    result_output = pd.DataFrame(data={\"id\": test[\"id\"], \"sentiment\": test_pred})\n",
    "    result_output.to_csv(\"/kaggle/working/distilbert_native.csv\", index=False, quoting=3)\n",
    "    logging.info('result saved!')\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8497927,
     "sourceId": 13392075,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8499338,
     "sourceId": 13393956,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8512611,
     "sourceId": 13412931,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3889.724268,
   "end_time": "2025-10-17T10:21:17.430232",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-10-17T09:16:27.705964",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
