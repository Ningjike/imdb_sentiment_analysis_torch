{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "372b5e92",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-10-24T15:28:19.178798Z",
     "iopub.status.busy": "2025-10-24T15:28:19.178436Z",
     "iopub.status.idle": "2025-10-24T15:29:04.979601Z",
     "shell.execute_reply": "2025-10-24T15:29:04.978844Z"
    },
    "papermill": {
     "duration": 45.808165,
     "end_time": "2025-10-24T15:29:04.980927",
     "exception": false,
     "start_time": "2025-10-24T15:28:19.172762",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 313/313 [00:03<00:00, 79.93it/s, epoch=0, train loss=0.6965, train acc=0.59, val loss=0.7294, val acc=0.52, time=3.92] \n",
      "Epoch 1: 100%|██████████| 313/313 [00:02<00:00, 110.89it/s, epoch=1, train loss=0.6040, train acc=0.69, val loss=0.5170, val acc=0.77, time=2.83]\n",
      "Epoch 2: 100%|██████████| 313/313 [00:02<00:00, 111.46it/s, epoch=2, train loss=0.5562, train acc=0.74, val loss=0.6530, val acc=0.67, time=2.81]\n",
      "Epoch 3: 100%|██████████| 313/313 [00:02<00:00, 111.34it/s, epoch=3, train loss=0.5362, train acc=0.75, val loss=0.5588, val acc=0.76, time=2.81]\n",
      "Epoch 4: 100%|██████████| 313/313 [00:02<00:00, 110.99it/s, epoch=4, train loss=0.4914, train acc=0.78, val loss=0.4938, val acc=0.78, time=2.82]\n",
      "Epoch 5: 100%|██████████| 313/313 [00:02<00:00, 110.53it/s, epoch=5, train loss=0.4108, train acc=0.82, val loss=0.4223, val acc=0.82, time=2.84]\n",
      "Epoch 6: 100%|██████████| 313/313 [00:02<00:00, 110.68it/s, epoch=6, train loss=0.4039, train acc=0.83, val loss=0.5221, val acc=0.79, time=2.83]\n",
      "Epoch 7: 100%|██████████| 313/313 [00:02<00:00, 110.46it/s, epoch=7, train loss=0.3929, train acc=0.83, val loss=0.6221, val acc=0.76, time=2.84]\n",
      "Epoch 8: 100%|██████████| 313/313 [00:02<00:00, 109.45it/s, epoch=8, train loss=0.3633, train acc=0.84, val loss=0.6452, val acc=0.71, time=2.86]\n",
      "Epoch 9: 100%|██████████| 313/313 [00:02<00:00, 109.30it/s, epoch=9, train loss=0.3084, train acc=0.88, val loss=0.6517, val acc=0.74, time=2.87]\n",
      "Prediction: 100%|██████████| 391/391 [00:01<00:00, 313.92it/s]\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "test = pd.read_csv(\"/kaggle/input/corpus-imdb/testData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\n",
    "num_epochs = 10\n",
    "embed_size = 300\n",
    "num_filter = 128\n",
    "filter_size = 3\n",
    "bidirectional = True\n",
    "batch_size = 64\n",
    "labels = 2\n",
    "lr = 0.8\n",
    "device = torch.device('cuda:0')\n",
    "use_gpu = True\n",
    "class SentimentNet(nn.Module):\n",
    "    def __init__(self, embed_size, num_filter, filter_size, weight, labels, use_gpu, **kwargs):\n",
    "        super(SentimentNet, self).__init__(**kwargs)\n",
    "        self.use_gpu = use_gpu\n",
    "        self.embedding = nn.Embedding.from_pretrained(weight)\n",
    "        self.embedding.weight.requires_grad = False\n",
    "\n",
    "        self.conv1d = nn.Conv1d(embed_size, num_filter, filter_size, padding=1)\n",
    "        self.activate = F.relu\n",
    "        self.decoder = nn.Linear(num_filter, labels)\n",
    "\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeddings = self.embedding(inputs)\n",
    "\n",
    "        convolution = self.activate(self.conv1d(embeddings.permute([0, 2, 1])))\n",
    "        pooling = F.max_pool1d(convolution, kernel_size=convolution.shape[2])\n",
    "\n",
    "        outputs = self.decoder(pooling.squeeze(dim=2))\n",
    "        # print(outputs)\n",
    "        return outputs\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    program = os.path.basename(sys.argv[0])\n",
    "    logger = logging.getLogger(program)\n",
    "\n",
    "    logging.basicConfig(format='%(asctime)s: %(levelname)s: %(message)s')\n",
    "    logging.root.setLevel(level=logging.INFO)\n",
    "    logger.info(r\"running %s\" % ''.join(sys.argv))\n",
    "\n",
    "    logging.info('loading data...')\n",
    "    pickle_file = os.path.join('/kaggle/input/pickle', 'imdb_glove.pickle3')\n",
    "    [train_features, train_labels, val_features, val_labels, test_features, weight, word_to_idx, idx_to_word,\n",
    "            vocab] = pickle.load(open(pickle_file, 'rb'))\n",
    "    logging.info('data loaded!')\n",
    "\n",
    "    net = SentimentNet(embed_size=embed_size, num_filter=num_filter, filter_size=filter_size,\n",
    "                       weight=weight, labels=labels, use_gpu=use_gpu)\n",
    "    net.to(device)\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=lr)\n",
    "\n",
    "    train_set = torch.utils.data.TensorDataset(train_features, train_labels)\n",
    "    val_set = torch.utils.data.TensorDataset(val_features, val_labels)\n",
    "    test_set = torch.utils.data.TensorDataset(test_features, )\n",
    "\n",
    "    train_iter = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "    val_iter = torch.utils.data.DataLoader(val_set, batch_size=batch_size, shuffle=False)\n",
    "    test_iter = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        start = time.time()\n",
    "        train_loss, val_losses = 0, 0\n",
    "        train_acc, val_acc = 0, 0\n",
    "        n, m = 0, 0\n",
    "        with tqdm(total=len(train_iter), desc='Epoch %d' % epoch) as pbar:\n",
    "            for feature, label in train_iter:\n",
    "                n += 1\n",
    "                net.zero_grad()\n",
    "                feature = Variable(feature.cuda())\n",
    "                label = Variable(label.cuda())\n",
    "                score = net(feature)\n",
    "                loss = loss_function(score, label)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_acc += accuracy_score(torch.argmax(score.cpu().data,\n",
    "                                                         dim=1), label.cpu())\n",
    "                train_loss += loss\n",
    "\n",
    "                pbar.set_postfix({'epoch': '%d' % (epoch),\n",
    "                                  'train loss': '%.4f' % (train_loss.data / n),\n",
    "                                  'train acc': '%.2f' % (train_acc / n)\n",
    "                                  })\n",
    "                pbar.update(1)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for val_feature, val_label in val_iter:\n",
    "                    m += 1\n",
    "                    val_feature = val_feature.cuda()\n",
    "                    val_label = val_label.cuda()\n",
    "                    val_score = net(val_feature)\n",
    "                    val_loss = loss_function(val_score, val_label)\n",
    "                    val_acc += accuracy_score(torch.argmax(val_score.cpu().data, dim=1), val_label.cpu())\n",
    "                    val_losses += val_loss\n",
    "            end = time.time()\n",
    "            runtime = end - start\n",
    "            pbar.set_postfix({'epoch': '%d' % (epoch),\n",
    "                              'train loss': '%.4f' % (train_loss.data / n),\n",
    "                              'train acc': '%.2f' % (train_acc / n),\n",
    "                              'val loss': '%.4f' % (val_losses.data / m),\n",
    "                              'val acc': '%.2f' % (val_acc / m),\n",
    "                              'time': '%.2f' % (runtime)})\n",
    "\n",
    "            # tqdm.write('{epoch: %d, train loss: %.4f, train acc: %.2f, val loss: %.4f, val acc: %.2f, time: %.2f}' %\n",
    "            #       (epoch, train_loss.data / n, train_acc / n, val_losses.data / m, val_acc / m, runtime))\n",
    "\n",
    "    test_pred = []\n",
    "    with torch.no_grad():\n",
    "        with tqdm(total=len(test_iter), desc='Prediction') as pbar:\n",
    "            for test_feature, in test_iter:\n",
    "                test_feature = test_feature.cuda()\n",
    "                test_score = net(test_feature)\n",
    "                # test_pred.extent\n",
    "                test_pred.extend(torch.argmax(test_score.cpu().data, dim=1).numpy().tolist())\n",
    "\n",
    "                pbar.update(1)\n",
    "\n",
    "    result_output = pd.DataFrame(data={\"id\": test[\"id\"], \"sentiment\": test_pred})\n",
    "    output_dir = \"/kaggle/working\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    result_output.to_csv(os.path.join(output_dir, \"cnn.csv\"), index=False, quoting=3)\n",
    "    logging.info('result saved!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23a9165a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T15:29:05.361240Z",
     "iopub.status.busy": "2025-10-24T15:29:05.360937Z",
     "iopub.status.idle": "2025-10-24T15:34:04.555306Z",
     "shell.execute_reply": "2025-10-24T15:34:04.554463Z"
    },
    "papermill": {
     "duration": 299.437392,
     "end_time": "2025-10-24T15:34:04.556556",
     "exception": false,
     "start_time": "2025-10-24T15:29:05.119164",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 313/313 [00:26<00:00, 11.61it/s, epoch=0, train loss=0.4364, train acc=0.79, val loss=0.2925, val acc=0.88, time=26.97]\n",
      "Epoch 1: 100%|██████████| 313/313 [00:27<00:00, 11.31it/s, epoch=1, train loss=0.2957, train acc=0.88, val loss=0.3115, val acc=0.89, time=27.68]\n",
      "Epoch 2: 100%|██████████| 313/313 [00:30<00:00, 10.31it/s, epoch=2, train loss=0.2508, train acc=0.90, val loss=0.2888, val acc=0.89, time=30.37]\n",
      "Epoch 3: 100%|██████████| 313/313 [00:28<00:00, 10.97it/s, epoch=3, train loss=0.2119, train acc=0.92, val loss=0.2916, val acc=0.89, time=28.55]\n",
      "Epoch 4: 100%|██████████| 313/313 [00:27<00:00, 11.25it/s, epoch=4, train loss=0.1773, train acc=0.93, val loss=0.3017, val acc=0.89, time=27.81]\n",
      "Epoch 5: 100%|██████████| 313/313 [00:28<00:00, 10.82it/s, epoch=5, train loss=0.1494, train acc=0.94, val loss=0.3549, val acc=0.89, time=28.93]\n",
      "Epoch 6: 100%|██████████| 313/313 [00:29<00:00, 10.76it/s, epoch=6, train loss=0.1380, train acc=0.95, val loss=0.3416, val acc=0.88, time=29.09]\n",
      "Epoch 7: 100%|██████████| 313/313 [00:28<00:00, 10.81it/s, epoch=7, train loss=0.1256, train acc=0.95, val loss=0.3517, val acc=0.87, time=28.97]\n",
      "Epoch 8: 100%|██████████| 313/313 [00:29<00:00, 10.73it/s, epoch=8, train loss=0.1114, train acc=0.96, val loss=0.3570, val acc=0.88, time=29.16]\n",
      "Epoch 9: 100%|██████████| 313/313 [00:29<00:00, 10.69it/s, epoch=9, train loss=0.1006, train acc=0.96, val loss=0.3982, val acc=0.88, time=29.28]\n",
      "Prediction: 100%|██████████| 391/391 [00:11<00:00, 34.75it/s]\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "test = pd.read_csv(\"/kaggle/input/corpus-imdb/testData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\n",
    "\n",
    "num_epochs = 10\n",
    "embed_size = 300\n",
    "num_hiddens = 120\n",
    "num_layers = 2\n",
    "bidirectional = True\n",
    "batch_size = 64\n",
    "labels = 2\n",
    "lr = 0.01\n",
    "device = torch.device('cuda:0')\n",
    "use_gpu = True\n",
    "\n",
    "\n",
    "class SentimentNet(nn.Module):\n",
    "    def __init__(self, embed_size, num_hiddens, num_layers, bidirectional, weight, labels, use_gpu, **kwargs):\n",
    "        super(SentimentNet, self).__init__(**kwargs)\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.num_layers = num_layers\n",
    "        self.use_gpu = use_gpu\n",
    "        self.bidirectional = bidirectional\n",
    "        self.embedding = nn.Embedding.from_pretrained(weight)\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        self.encoder = nn.LSTM(input_size=embed_size, hidden_size=self.num_hiddens,\n",
    "                               num_layers=num_layers, bidirectional=self.bidirectional,\n",
    "                               dropout=0)\n",
    "        if self.bidirectional:\n",
    "            self.decoder = nn.Linear(num_hiddens * 4, labels)\n",
    "        else:\n",
    "            self.decoder = nn.Linear(num_hiddens * 2, labels)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeddings = self.embedding(inputs)\n",
    "        states, hidden = self.encoder(embeddings.permute([1, 0, 2]))\n",
    "        encoding = torch.cat([states[0], states[-1]], dim=1)\n",
    "        outputs = self.decoder(encoding)\n",
    "        # print(outputs)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    program = os.path.basename(sys.argv[0])\n",
    "    logger = logging.getLogger(program)\n",
    "\n",
    "    logging.basicConfig(format='%(asctime)s: %(levelname)s: %(message)s')\n",
    "    logging.root.setLevel(level=logging.INFO)\n",
    "    logger.info(r\"running %s\" % ''.join(sys.argv))\n",
    "\n",
    "    logging.info('loading data...')\n",
    "    pickle_file = os.path.join('/kaggle/input/pickle', 'imdb_glove.pickle3')\n",
    "    [train_features, train_labels, val_features, val_labels, test_features, weight, word_to_idx, idx_to_word,\n",
    "            vocab] = pickle.load(open(pickle_file, 'rb'))\n",
    "    logging.info('data loaded!')\n",
    "\n",
    "    net = SentimentNet(embed_size=embed_size, num_hiddens=num_hiddens, num_layers=num_layers,\n",
    "                       bidirectional=bidirectional, weight=weight,\n",
    "                       labels=labels, use_gpu=use_gpu)\n",
    "    net.to(device)\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "    train_set = torch.utils.data.TensorDataset(train_features, train_labels)\n",
    "    val_set = torch.utils.data.TensorDataset(val_features, val_labels)\n",
    "    test_set = torch.utils.data.TensorDataset(test_features, )\n",
    "\n",
    "    train_iter = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "    val_iter = torch.utils.data.DataLoader(val_set, batch_size=batch_size, shuffle=False)\n",
    "    test_iter = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        start = time.time()\n",
    "        train_loss, val_losses = 0, 0\n",
    "        train_acc, val_acc = 0, 0\n",
    "        n, m = 0, 0\n",
    "        with tqdm(total=len(train_iter), desc='Epoch %d' % epoch) as pbar:\n",
    "            for feature, label in train_iter:\n",
    "                n += 1\n",
    "                net.zero_grad()\n",
    "                feature = Variable(feature.cuda())\n",
    "                label = Variable(label.cuda())\n",
    "                score = net(feature)\n",
    "                loss = loss_function(score, label)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_acc += accuracy_score(torch.argmax(score.cpu().data,\n",
    "                                                         dim=1), label.cpu())\n",
    "                train_loss += loss\n",
    "\n",
    "                pbar.set_postfix({'epoch': '%d' % (epoch),\n",
    "                                  'train loss': '%.4f' % (train_loss.data / n),\n",
    "                                  'train acc': '%.2f' % (train_acc / n)\n",
    "                                  })\n",
    "                pbar.update(1)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for val_feature, val_label in val_iter:\n",
    "                    m += 1\n",
    "                    val_feature = val_feature.cuda()\n",
    "                    val_label = val_label.cuda()\n",
    "                    val_score = net(val_feature)\n",
    "                    val_loss = loss_function(val_score, val_label)\n",
    "                    val_acc += accuracy_score(torch.argmax(val_score.cpu().data, dim=1), val_label.cpu())\n",
    "                    val_losses += val_loss\n",
    "            end = time.time()\n",
    "            runtime = end - start\n",
    "            pbar.set_postfix({'epoch': '%d' % (epoch),\n",
    "                              'train loss': '%.4f' % (train_loss.data / n),\n",
    "                              'train acc': '%.2f' % (train_acc / n),\n",
    "                              'val loss': '%.4f' % (val_losses.data / m),\n",
    "                              'val acc': '%.2f' % (val_acc / m),\n",
    "                              'time': '%.2f' % (runtime)\n",
    "                              })\n",
    "\n",
    "            # tqdm.write('{epoch: %d, train loss: %.4f, train acc: %.2f, val loss: %.4f, val acc: %.2f, time: %.2f}' %\n",
    "            #       (epoch, train_loss.data / n, train_acc / n, val_losses.data / m, val_acc / m, runtime))\n",
    "\n",
    "    test_pred = []\n",
    "    with torch.no_grad():\n",
    "        with tqdm(total=len(test_iter), desc='Prediction') as pbar:\n",
    "            for test_feature, in test_iter:\n",
    "                test_feature = test_feature.cuda()\n",
    "                test_score = net(test_feature)\n",
    "                # test_pred.extent\n",
    "                test_pred.extend(torch.argmax(test_score.cpu().data, dim=1).numpy().tolist())\n",
    "\n",
    "                pbar.update(1)\n",
    "\n",
    "    result_output = pd.DataFrame(data={\"id\": test[\"id\"], \"sentiment\": test_pred})\n",
    "    result_output.to_csv(\"/kaggle/working/lstm.csv\", index=False, quoting=3)\n",
    "    logging.info('result saved!')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bc984f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T15:34:05.214015Z",
     "iopub.status.busy": "2025-10-24T15:34:05.213678Z",
     "iopub.status.idle": "2025-10-24T15:37:37.814154Z",
     "shell.execute_reply": "2025-10-24T15:37:37.813332Z"
    },
    "papermill": {
     "duration": 212.923852,
     "end_time": "2025-10-24T15:37:37.815289",
     "exception": false,
     "start_time": "2025-10-24T15:34:04.891437",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 313/313 [00:20<00:00, 15.33it/s, epoch=0, train loss=0.6871, train acc=0.56, val loss=0.6418, val acc=0.64, time=20.42]\n",
      "Epoch 1: 100%|██████████| 313/313 [00:20<00:00, 15.31it/s, epoch=1, train loss=0.4798, train acc=0.77, val loss=0.4830, val acc=0.79, time=20.44]\n",
      "Epoch 2: 100%|██████████| 313/313 [00:20<00:00, 15.31it/s, epoch=2, train loss=0.4230, train acc=0.81, val loss=0.3893, val acc=0.83, time=20.44]\n",
      "Epoch 3: 100%|██████████| 313/313 [00:20<00:00, 15.33it/s, epoch=3, train loss=0.3996, train acc=0.82, val loss=0.4476, val acc=0.82, time=20.43]\n",
      "Epoch 4: 100%|██████████| 313/313 [00:20<00:00, 15.32it/s, epoch=4, train loss=0.3622, train acc=0.84, val loss=0.6398, val acc=0.73, time=20.43]\n",
      "Epoch 5: 100%|██████████| 313/313 [00:20<00:00, 15.30it/s, epoch=5, train loss=0.3423, train acc=0.85, val loss=0.4378, val acc=0.81, time=20.46]\n",
      "Epoch 6: 100%|██████████| 313/313 [00:20<00:00, 15.33it/s, epoch=6, train loss=0.3208, train acc=0.87, val loss=0.3349, val acc=0.86, time=20.43]\n",
      "Epoch 7: 100%|██████████| 313/313 [00:20<00:00, 15.33it/s, epoch=7, train loss=0.3043, train acc=0.87, val loss=0.3526, val acc=0.85, time=20.42]\n",
      "Epoch 8: 100%|██████████| 313/313 [00:20<00:00, 15.31it/s, epoch=8, train loss=0.2921, train acc=0.88, val loss=0.4031, val acc=0.85, time=20.45]\n",
      "Epoch 9: 100%|██████████| 313/313 [00:20<00:00, 15.33it/s, epoch=9, train loss=0.2638, train acc=0.89, val loss=0.3278, val acc=0.86, time=20.43]\n",
      "Prediction: 100%|██████████| 391/391 [00:07<00:00, 54.49it/s]\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "test = pd.read_csv(\"/kaggle/input/corpus-imdb/testData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\n",
    "\n",
    "num_epochs = 10\n",
    "embed_size = 300\n",
    "num_hiddens = 120\n",
    "num_layers = 2\n",
    "bidirectional = True\n",
    "batch_size = 64\n",
    "labels = 2\n",
    "lr = 0.8\n",
    "device = torch.device('cuda:0')\n",
    "use_gpu = True\n",
    "\n",
    "\n",
    "class SentimentNet(nn.Module):\n",
    "    def __init__(self, embed_size, num_hiddens, num_layers, bidirectional, weight, labels, use_gpu, **kwargs):\n",
    "        super(SentimentNet, self).__init__(**kwargs)\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.num_layers = num_layers\n",
    "        self.use_gpu = use_gpu\n",
    "        self.bidirectional = bidirectional\n",
    "        self.embedding = nn.Embedding.from_pretrained(weight)\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        self.encoder = nn.GRU(input_size=embed_size, hidden_size=self.num_hiddens,\n",
    "                               num_layers=num_layers, bidirectional=self.bidirectional, dropout=0)\n",
    "        if self.bidirectional:\n",
    "            self.decoder = nn.Linear(num_hiddens * 4, labels)\n",
    "        else:\n",
    "            self.decoder = nn.Linear(num_hiddens * 2, labels)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeddings = self.embedding(inputs)\n",
    "        states, hidden = self.encoder(embeddings.permute([1, 0, 2]))\n",
    "        encoding = torch.cat([states[0], states[-1]], dim=1)\n",
    "        outputs = self.decoder(encoding)\n",
    "        # print(outputs)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    program = os.path.basename(sys.argv[0])\n",
    "    logger = logging.getLogger(program)\n",
    "\n",
    "    logging.basicConfig(format='%(asctime)s: %(levelname)s: %(message)s')\n",
    "    logging.root.setLevel(level=logging.INFO)\n",
    "    logger.info(r\"running %s\" % ''.join(sys.argv))\n",
    "\n",
    "    logging.info('loading data...')\n",
    "    pickle_file = os.path.join('/kaggle/input/pickle', 'imdb_glove.pickle3')\n",
    "    [train_features, train_labels, val_features, val_labels, test_features, weight, word_to_idx, idx_to_word,\n",
    "            vocab] = pickle.load(open(pickle_file, 'rb'))\n",
    "    logging.info('data loaded!')\n",
    "\n",
    "\n",
    "    net = SentimentNet(embed_size=embed_size, num_hiddens=num_hiddens, num_layers=num_layers,\n",
    "                       bidirectional=bidirectional, weight=weight,\n",
    "                       labels=labels, use_gpu=use_gpu)\n",
    "    net.to(device)\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=lr)\n",
    "\n",
    "    train_set = torch.utils.data.TensorDataset(train_features, train_labels)\n",
    "    val_set = torch.utils.data.TensorDataset(val_features, val_labels)\n",
    "    test_set = torch.utils.data.TensorDataset(test_features, )\n",
    "\n",
    "    train_iter = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "    val_iter = torch.utils.data.DataLoader(val_set, batch_size=batch_size, shuffle=False)\n",
    "    test_iter = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        start = time.time()\n",
    "        train_loss, val_losses = 0, 0\n",
    "        train_acc, val_acc = 0, 0\n",
    "        n, m = 0, 0\n",
    "        with tqdm(total=len(train_iter), desc='Epoch %d' % epoch) as pbar:\n",
    "            for feature, label in train_iter:\n",
    "                n += 1\n",
    "                net.zero_grad()\n",
    "                feature = Variable(feature.cuda())\n",
    "                label = Variable(label.cuda())\n",
    "                score = net(feature)\n",
    "                loss = loss_function(score, label)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_acc += accuracy_score(torch.argmax(score.cpu().data,\n",
    "                                                         dim=1), label.cpu())\n",
    "                train_loss += loss\n",
    "\n",
    "                pbar.set_postfix({'epoch': '%d' % (epoch),\n",
    "                                  'train loss': '%.4f' % (train_loss.data / n),\n",
    "                                  'train acc': '%.2f' % (train_acc / n)\n",
    "                                  })\n",
    "                pbar.update(1)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for val_feature, val_label in val_iter:\n",
    "                    m += 1\n",
    "                    val_feature = val_feature.cuda()\n",
    "                    val_label = val_label.cuda()\n",
    "                    val_score = net(val_feature)\n",
    "                    val_loss = loss_function(val_score, val_label)\n",
    "                    val_acc += accuracy_score(torch.argmax(val_score.cpu().data, dim=1), val_label.cpu())\n",
    "                    val_losses += val_loss\n",
    "            end = time.time()\n",
    "            runtime = end - start\n",
    "            pbar.set_postfix({'epoch': '%d' % (epoch),\n",
    "                              'train loss': '%.4f' % (train_loss.data / n),\n",
    "                              'train acc': '%.2f' % (train_acc / n),\n",
    "                              'val loss': '%.4f' % (val_losses.data / m),\n",
    "                              'val acc': '%.2f' % (val_acc / m),\n",
    "                              'time': '%.2f' % (runtime)})\n",
    "\n",
    "            # tqdm.write('{epoch: %d, train loss: %.4f, train acc: %.2f, val loss: %.4f, val acc: %.2f, time: %.2f}' %\n",
    "            #       (epoch, train_loss.data / n, train_acc / n, val_losses.data / m, val_acc / m, runtime))\n",
    "\n",
    "    test_pred = []\n",
    "    with torch.no_grad():\n",
    "        with tqdm(total=len(test_iter), desc='Prediction') as pbar:\n",
    "            for test_feature, in test_iter:\n",
    "                test_feature = test_feature.cuda()\n",
    "                test_score = net(test_feature)\n",
    "                # test_pred.extent\n",
    "                test_pred.extend(torch.argmax(test_score.cpu().data, dim=1).numpy().tolist())\n",
    "\n",
    "                pbar.update(1)\n",
    "\n",
    "    result_output = pd.DataFrame(data={\"id\": test[\"id\"], \"sentiment\": test_pred})\n",
    "    result_output.to_csv(\"/kaggle/working/gru.csv\", index=False, quoting=3)\n",
    "    logging.info('result saved!')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbc5b5f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T15:37:38.839694Z",
     "iopub.status.busy": "2025-10-24T15:37:38.839439Z",
     "iopub.status.idle": "2025-10-24T15:39:11.813348Z",
     "shell.execute_reply": "2025-10-24T15:39:11.812582Z"
    },
    "papermill": {
     "duration": 93.487061,
     "end_time": "2025-10-24T15:39:11.814432",
     "exception": false,
     "start_time": "2025-10-24T15:37:38.327371",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n",
      "Epoch 0: 100%|██████████| 313/313 [00:06<00:00, 47.40it/s, epoch=0, train loss=0.6962, train acc=0.52, val loss=0.6963, val acc=0.51, time=6.61]\n",
      "Epoch 1: 100%|██████████| 313/313 [00:06<00:00, 50.30it/s, epoch=1, train loss=0.6895, train acc=0.54, val loss=0.6942, val acc=0.52, time=6.24]\n",
      "Epoch 2: 100%|██████████| 313/313 [00:06<00:00, 49.99it/s, epoch=2, train loss=0.6869, train acc=0.54, val loss=0.6854, val acc=0.54, time=6.28]\n",
      "Epoch 3: 100%|██████████| 313/313 [00:06<00:00, 49.17it/s, epoch=3, train loss=0.6844, train acc=0.55, val loss=0.6821, val acc=0.56, time=6.38]\n",
      "Epoch 4: 100%|██████████| 313/313 [00:06<00:00, 48.39it/s, epoch=4, train loss=0.6824, train acc=0.56, val loss=0.6842, val acc=0.56, time=6.49]\n",
      "Epoch 5: 100%|██████████| 313/313 [00:06<00:00, 49.17it/s, epoch=5, train loss=0.6808, train acc=0.56, val loss=0.6792, val acc=0.57, time=6.38]\n",
      "Epoch 6: 100%|██████████| 313/313 [00:06<00:00, 49.76it/s, epoch=6, train loss=0.6782, train acc=0.56, val loss=0.6796, val acc=0.57, time=6.31]\n",
      "Epoch 7: 100%|██████████| 313/313 [00:06<00:00, 50.28it/s, epoch=7, train loss=0.6774, train acc=0.57, val loss=0.6838, val acc=0.55, time=6.24]\n",
      "Epoch 8: 100%|██████████| 313/313 [00:06<00:00, 50.55it/s, epoch=8, train loss=0.6753, train acc=0.58, val loss=0.6765, val acc=0.58, time=6.21]\n",
      "Epoch 9: 100%|██████████| 313/313 [00:06<00:00, 50.62it/s, epoch=9, train loss=0.6749, train acc=0.57, val loss=0.6798, val acc=0.57, time=6.20]\n",
      "Prediction: 100%|██████████| 391/391 [00:02<00:00, 158.86it/s]\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict, Counter\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "num_epochs = 10\n",
    "embed_size = 120\n",
    "num_hiddens = 120\n",
    "num_layers = 2\n",
    "bidirectional = True\n",
    "batch_size = 64\n",
    "labels = 2\n",
    "lr = 0.0001\n",
    "device = torch.device('cuda:0')\n",
    "use_gpu = True\n",
    "\n",
    "\n",
    "# Read data from files\n",
    "train = pd.read_csv(\"/kaggle/input/corpus-imdb/labeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\n",
    "test = pd.read_csv(\"/kaggle/input/corpus-imdb/testData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\n",
    "\n",
    "\n",
    "def review_to_wordlist(review, remove_stopwords=False):\n",
    "    # Function to convert a document to a sequence of words,\n",
    "    # optionally removing stop words.  Returns a list of words.\n",
    "    #\n",
    "    # 1. Remove HTML\n",
    "    review_text = BeautifulSoup(review, \"lxml\").get_text()\n",
    "    #\n",
    "    # 2. Remove non-letters\n",
    "    review_text = re.sub(\"[^a-zA-Z]\", \" \", review_text)\n",
    "    #\n",
    "    # 3. Convert words to lower case and split them\n",
    "    words = review_text.lower().split()\n",
    "    #\n",
    "    # 4. Optionally remove stop words (false by default)\n",
    "    # if remove_stopwords:\n",
    "    #     stops = set(stopwords.words(\"english\"))\n",
    "    #     words = [w for w in words if not w in stops]\n",
    "    #\n",
    "    # 5. Return a list of words\n",
    "    return ' '.join(words)\n",
    "\n",
    "\n",
    "class Vocab:\n",
    "    def __init__(self, tokens=None):\n",
    "        self.idx_to_token = list()\n",
    "        self.token_to_idx = dict()\n",
    "\n",
    "        if tokens is not None:\n",
    "            if \"<unk>\" not in tokens:\n",
    "                tokens = tokens + [\"<unk>\"]\n",
    "            for token in tokens:\n",
    "                self.idx_to_token.append(token)\n",
    "                self.token_to_idx[token] = len(self.idx_to_token) - 1\n",
    "            self.unk = self.token_to_idx['<unk>']\n",
    "\n",
    "    @classmethod\n",
    "    def build(cls, train, test, min_freq=1, reserved_tokens=None):\n",
    "        token_freqs = defaultdict(int)\n",
    "        for sentence in train:\n",
    "            for token in sentence:\n",
    "                token_freqs[token] += 1\n",
    "\n",
    "        for sentence in test:\n",
    "            for token in sentence:\n",
    "                token_freqs[token] += 1\n",
    "\n",
    "        uniq_tokens = [\"<unk>\"] + (reserved_tokens if reserved_tokens else [])\n",
    "        uniq_tokens += [token for token, freq in token_freqs.items() \\\n",
    "                        if freq >= min_freq and token != \"<unk>\"]\n",
    "        return cls(uniq_tokens)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)\n",
    "\n",
    "    def __getitem__(self, token):\n",
    "        return self.token_to_idx.get(token, self.unk)\n",
    "\n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        return [self[token] for token in tokens]\n",
    "\n",
    "    def convert_ids_to_tokens(self, indices):\n",
    "        return [self.idx_to_token[index] for index in indices]\n",
    "\n",
    "\n",
    "def length_to_mask(lengths):\n",
    "    max_length = torch.max(lengths)\n",
    "    mask = torch.arange(max_length).to(lengths.device).expand(lengths.shape[0], max_length) < lengths.unsqueeze(1)\n",
    "    return mask\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=512):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return x\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_class,\n",
    "                 dim_feedforward=512, num_head=2, num_layers=2, dropout=0.1, max_len=512, activation: str = \"relu\"):\n",
    "        super(Transformer, self).__init__()\n",
    "        # 词嵌入层\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.position_embedding = PositionalEncoding(embedding_dim, dropout, max_len)\n",
    "        # 编码层：使用Transformer\n",
    "        encoder_layer = nn.TransformerEncoderLayer(hidden_dim, num_head, dim_feedforward, dropout, activation)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        # 输出层\n",
    "        self.output = nn.Linear(hidden_dim, num_class)\n",
    "\n",
    "    def forward(self, inputs, lengths):\n",
    "        inputs = torch.transpose(inputs, 0, 1)\n",
    "        hidden_states = self.embeddings(inputs)\n",
    "        hidden_states = self.position_embedding(hidden_states)\n",
    "        attention_mask = length_to_mask(lengths) == False\n",
    "        hidden_states = self.transformer(hidden_states, src_key_padding_mask=attention_mask)\n",
    "        hidden_states = hidden_states[0, :, :]\n",
    "        output = self.output(hidden_states)\n",
    "        log_probs = F.log_softmax(output, dim=1)\n",
    "        return log_probs\n",
    "\n",
    "\n",
    "class TransformerDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.data[i]\n",
    "\n",
    "\n",
    "# def collate_fn(examples, max_length = 128) :\n",
    "#     lengths = torch.tensor([min(len(ex[0]), max_length) for ex in examples])\n",
    "#     inputs = [torch.tensor(ex[0][:max_length]) for ex in examples]\n",
    "#     targets = torch.tensor([ex[1] for ex in examples], dtype=torch.long)\n",
    "#     # 对batch内的样本进行padding，使其具有相同长度\n",
    "#     inputs = pad_sequence(inputs, batch_first=True)\n",
    "#     return inputs, lengths, targets    \n",
    "def collate_fn_train(examples, max_length=128):\n",
    "    lengths = torch.tensor([min(len(ex[0]), max_length) for ex in examples])\n",
    "    inputs = [torch.tensor(ex[0][:max_length]) for ex in examples]\n",
    "    targets = torch.tensor([ex[1] for ex in examples], dtype=torch.long)\n",
    "    inputs = pad_sequence(inputs, batch_first=True)\n",
    "    return inputs, lengths, targets\n",
    "\n",
    "\n",
    "def collate_fn_test(examples, max_length=128):\n",
    "    lengths = torch.tensor([min(len(ex), max_length) for ex in examples])\n",
    "    inputs = [torch.tensor(ex[:max_length]) for ex in examples]\n",
    "    inputs = pad_sequence(inputs, batch_first=True)\n",
    "    return inputs, lengths  # 只返回 inputs 和 lengths\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    program = os.path.basename(sys.argv[0])\n",
    "    logger = logging.getLogger(program)\n",
    "\n",
    "    logging.basicConfig(format='%(asctime)s: %(levelname)s: %(message)s')\n",
    "    logging.root.setLevel(level=logging.INFO)\n",
    "    logger.info(r\"running %s\" % ''.join(sys.argv))\n",
    "\n",
    "    clean_train_reviews, train_labels = [], []\n",
    "    for i, review in enumerate(train[\"review\"]):\n",
    "        clean_train_reviews.append(review_to_wordlist(review, remove_stopwords=False))\n",
    "        train_labels.append(train[\"sentiment\"][i])\n",
    "\n",
    "    clean_test_reviews = []\n",
    "    for review in test[\"review\"]:\n",
    "        clean_test_reviews.append(review_to_wordlist(review, remove_stopwords=False))\n",
    "\n",
    "    vocab = Vocab.build(clean_train_reviews, clean_test_reviews)\n",
    "\n",
    "    train_reviews = [(vocab.convert_tokens_to_ids(sentence), train_labels[i])\n",
    "                     for i, sentence in enumerate(clean_train_reviews)]\n",
    "    test_reviews = [vocab.convert_tokens_to_ids(sentence)\n",
    "                     for sentence in clean_test_reviews]\n",
    "\n",
    "    train_reviews, val_reviews, train_labels, val_labels = train_test_split(train_reviews, train_labels,\n",
    "                                                                            test_size=0.2, random_state=0)\n",
    "\n",
    "    net = Transformer(vocab_size=len(vocab), embedding_dim=embed_size, hidden_dim=num_hiddens, num_class=labels)\n",
    "    net.to(device)\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "    train_set = TransformerDataset(train_reviews)\n",
    "    val_set = TransformerDataset(val_reviews)\n",
    "    test_set = TransformerDataset(test_reviews)\n",
    "\n",
    "    train_iter = torch.utils.data.DataLoader(train_set, collate_fn=collate_fn_train, batch_size=batch_size, shuffle=True)\n",
    "    val_iter = torch.utils.data.DataLoader(val_set, collate_fn=collate_fn_train, batch_size=batch_size, shuffle=False)\n",
    "    test_iter = torch.utils.data.DataLoader(test_set, collate_fn=collate_fn_test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        start = time.time()\n",
    "        train_loss, val_losses = 0, 0\n",
    "        train_acc, val_acc = 0, 0\n",
    "        n, m = 0, 0\n",
    "        with tqdm(total=len(train_iter), desc='Epoch %d' % epoch) as pbar:\n",
    "            for feature, lengths, label in train_iter:\n",
    "                # print(feature, lengths, label)\n",
    "                n += 1\n",
    "                net.zero_grad()\n",
    "                feature = Variable(feature.cuda())\n",
    "                lengths = Variable(lengths.cuda())\n",
    "                label = Variable(label.cuda())\n",
    "                score = net(feature, lengths)\n",
    "                loss = loss_function(score, label)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_acc += accuracy_score(torch.argmax(score.cpu().data,\n",
    "                                                         dim=1), label.cpu())\n",
    "                train_loss += loss\n",
    "\n",
    "                pbar.set_postfix({'epoch': '%d' % (epoch),\n",
    "                                  'train loss': '%.4f' % (train_loss.data / n),\n",
    "                                  'train acc': '%.2f' % (train_acc / n)\n",
    "                                  })\n",
    "                pbar.update(1)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for val_feature, val_length, val_label in val_iter:\n",
    "                    m += 1\n",
    "                    val_feature = val_feature.cuda()\n",
    "                    val_length = val_length.cuda()\n",
    "                    val_label = val_label.cuda()\n",
    "                    val_score = net(val_feature, val_length)\n",
    "                    val_loss = loss_function(val_score, val_label)\n",
    "                    val_acc += accuracy_score(torch.argmax(val_score.cpu().data, dim=1), val_label.cpu())\n",
    "                    val_losses += val_loss\n",
    "            end = time.time()\n",
    "            runtime = end - start\n",
    "            pbar.set_postfix({'epoch': '%d' % (epoch),\n",
    "                              'train loss': '%.4f' % (train_loss.data / n),\n",
    "                              'train acc': '%.2f' % (train_acc / n),\n",
    "                              'val loss': '%.4f' % (val_losses.data / m),\n",
    "                              'val acc': '%.2f' % (val_acc / m),\n",
    "                              'time': '%.2f' % (runtime)\n",
    "                              })\n",
    "\n",
    "            # tqdm.write('{epoch: %d, train loss: %.4f, train acc: %.2f, val loss: %.4f, val acc: %.2f, time: %.2f}' %\n",
    "            #       (epoch, train_loss.data / n, train_acc / n, val_losses.data / m, val_acc / m, runtime))\n",
    "\n",
    "    test_pred = []\n",
    "    with torch.no_grad():\n",
    "        with tqdm(total=len(test_iter), desc='Prediction') as pbar:\n",
    "            for test_feature, test_length in test_iter:\n",
    "                test_feature = test_feature.cuda()\n",
    "                test_length = test_length.cuda()\n",
    "                test_score = net(test_feature, test_length)  # 确保传入 lengths\n",
    "                test_pred.extend(torch.argmax(test_score.cpu().data, dim=1).numpy().tolist())\n",
    "                pbar.update(1)\n",
    "    result_output = pd.DataFrame(data={\"id\": test[\"id\"], \"sentiment\": test_pred})\n",
    "    result_output.to_csv(\"/kaggle/working/transformer.csv\", index=False, quoting=3)\n",
    "    logging.info('result saved!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "301aaa68",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T15:39:13.222577Z",
     "iopub.status.busy": "2025-10-24T15:39:13.222319Z",
     "iopub.status.idle": "2025-10-24T15:44:45.073055Z",
     "shell.execute_reply": "2025-10-24T15:44:45.072163Z"
    },
    "papermill": {
     "duration": 332.515952,
     "end_time": "2025-10-24T15:44:45.074229",
     "exception": false,
     "start_time": "2025-10-24T15:39:12.558277",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 313/313 [00:31<00:00, 10.05it/s, epoch=0, train loss=0.6933, train acc=0.52, val loss=0.6894, val acc=0.55, time=31.17]\n",
      "Epoch 1: 100%|██████████| 313/313 [00:32<00:00,  9.54it/s, epoch=1, train loss=0.6850, train acc=0.54, val loss=0.6971, val acc=0.58, time=32.82]\n",
      "Epoch 2: 100%|██████████| 313/313 [00:31<00:00,  9.94it/s, epoch=2, train loss=0.5345, train acc=0.76, val loss=0.4828, val acc=0.80, time=31.50]\n",
      "Epoch 3: 100%|██████████| 313/313 [00:31<00:00,  9.88it/s, epoch=3, train loss=0.4744, train acc=0.79, val loss=0.4025, val acc=0.83, time=31.70]\n",
      "Epoch 4: 100%|██████████| 313/313 [00:32<00:00,  9.78it/s, epoch=4, train loss=0.3920, train acc=0.84, val loss=0.3915, val acc=0.83, time=32.01]\n",
      "Epoch 5: 100%|██████████| 313/313 [00:31<00:00,  9.86it/s, epoch=5, train loss=0.3705, train acc=0.85, val loss=0.4107, val acc=0.85, time=31.75]\n",
      "Epoch 6: 100%|██████████| 313/313 [00:31<00:00,  9.95it/s, epoch=6, train loss=0.3458, train acc=0.86, val loss=0.3474, val acc=0.86, time=31.47]\n",
      "Epoch 7: 100%|██████████| 313/313 [00:31<00:00,  9.86it/s, epoch=7, train loss=0.3137, train acc=0.87, val loss=0.3565, val acc=0.85, time=31.75]\n",
      "Epoch 8: 100%|██████████| 313/313 [00:31<00:00,  9.81it/s, epoch=8, train loss=0.2960, train acc=0.89, val loss=0.3624, val acc=0.87, time=31.92]\n",
      "Epoch 9: 100%|██████████| 313/313 [00:31<00:00,  9.80it/s, epoch=9, train loss=0.2810, train acc=0.89, val loss=0.3581, val acc=0.87, time=31.95]\n",
      "Prediction: 100%|██████████| 391/391 [00:12<00:00, 31.92it/s]\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "test = pd.read_csv(\"/kaggle/input/corpus-imdb/testData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\n",
    "\n",
    "num_epochs = 10\n",
    "embed_size = 300\n",
    "num_hiddens = 128\n",
    "num_layers = 2\n",
    "bidirectional = True\n",
    "batch_size = 64\n",
    "labels = 2\n",
    "lr = 0.01\n",
    "device = torch.device('cuda:0')\n",
    "use_gpu = True\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, num_hiddens, bidirectional, **kwargs):\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.bidirectional = bidirectional\n",
    "\n",
    "        # if bidirectional, then double the hidden dimensionality\n",
    "        if self.bidirectional:\n",
    "            self.w_omega = nn.Parameter(torch.Tensor(num_hiddens * 2, num_hiddens * 2))\n",
    "            self.u_omega = nn.Parameter(torch.Tensor(num_hiddens * 2, 1))\n",
    "        else:\n",
    "            self.w_omega = nn.Parameter(torch.Tensor(num_hiddens, num_hiddens))\n",
    "            self.u_omega = nn.Parameter(torch.Tensor(num_hiddens, 1))\n",
    "\n",
    "        nn.init.uniform_(self.w_omega, -0.1, 0.1)\n",
    "        nn.init.uniform_(self.u_omega, -0.1, 0.1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = inputs\n",
    "        u = torch.tanh(torch.matmul(x, self.w_omega))\n",
    "        att = torch.matmul(u, self.u_omega)\n",
    "\n",
    "        att_score = F.softmax(att, dim=1)\n",
    "        outputs = x * att_score\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class SentimentNet(nn.Module):\n",
    "    def __init__(self, embed_size, num_hiddens, num_layers, bidirectional, weight, labels, use_gpu, **kwargs):\n",
    "        super(SentimentNet, self).__init__(**kwargs)\n",
    "        self.embed_size = embed_size\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.num_layers = num_layers\n",
    "        self.use_gpu = use_gpu\n",
    "        self.bidirectional = bidirectional\n",
    "        self.embedding = nn.Embedding.from_pretrained(weight)\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        self.encoder = nn.LSTM(input_size=self.embed_size, hidden_size=self.num_hiddens,\n",
    "                               num_layers=self.num_layers, bidirectional=self.bidirectional,\n",
    "                               dropout=0)\n",
    "        self.attention = Attention(num_hiddens=self.num_hiddens, bidirectional=self.bidirectional)\n",
    "        if self.bidirectional:\n",
    "            self.decoder = nn.Linear(num_hiddens * 4, labels)\n",
    "        else:\n",
    "            self.decoder = nn.Linear(num_hiddens * 2, labels)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeddings = self.embedding(inputs)\n",
    "        states, hidden = self.encoder(embeddings.permute(1, 0, 2))\n",
    "        attention = self.attention(states)\n",
    "        encoding = torch.cat([attention[0], attention[-1]], dim=1)\n",
    "        outputs = self.decoder(encoding)\n",
    "        # print(outputs)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    program = os.path.basename(sys.argv[0])\n",
    "    logger = logging.getLogger(program)\n",
    "\n",
    "    logging.basicConfig(format='%(asctime)s: %(levelname)s: %(message)s')\n",
    "    logging.root.setLevel(level=logging.INFO)\n",
    "    logger.info(r\"running %s\" % ''.join(sys.argv))\n",
    "\n",
    "    logging.info('loading data...')\n",
    "    pickle_file = os.path.join('/kaggle/input/pickle', 'imdb_glove.pickle3')\n",
    "    [train_features, train_labels, val_features, val_labels, test_features, weight, word_to_idx, idx_to_word,\n",
    "            vocab] = pickle.load(open(pickle_file, 'rb'))\n",
    "    logging.info('data loaded!')\n",
    "\n",
    "    net = SentimentNet(embed_size=embed_size, num_hiddens=num_hiddens, num_layers=num_layers,\n",
    "                       bidirectional=bidirectional, weight=weight,\n",
    "                       labels=labels, use_gpu=use_gpu)\n",
    "    net.to(device)\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "    train_set = torch.utils.data.TensorDataset(train_features, train_labels)\n",
    "    val_set = torch.utils.data.TensorDataset(val_features, val_labels)\n",
    "    test_set = torch.utils.data.TensorDataset(test_features, )\n",
    "\n",
    "    train_iter = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "    val_iter = torch.utils.data.DataLoader(val_set, batch_size=batch_size, shuffle=False)\n",
    "    test_iter = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        start = time.time()\n",
    "        train_loss, val_losses = 0, 0\n",
    "        train_acc, val_acc = 0, 0\n",
    "        n, m = 0, 0\n",
    "        with tqdm(total=len(train_iter), desc='Epoch %d' % epoch) as pbar:\n",
    "            for feature, label in train_iter:\n",
    "                n += 1\n",
    "                net.zero_grad()\n",
    "                feature = Variable(feature.cuda())\n",
    "                label = Variable(label.cuda())\n",
    "                score = net(feature)\n",
    "                loss = loss_function(score, label)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_acc += accuracy_score(torch.argmax(score.cpu().data,\n",
    "                                                         dim=1), label.cpu())\n",
    "                train_loss += loss\n",
    "\n",
    "                pbar.set_postfix({'epoch': '%d' % (epoch),\n",
    "                                  'train loss': '%.4f' % (train_loss.data / n),\n",
    "                                  'train acc': '%.2f' % (train_acc / n)\n",
    "                                  })\n",
    "                pbar.update(1)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for val_feature, val_label in val_iter:\n",
    "                    m += 1\n",
    "                    val_feature = val_feature.cuda()\n",
    "                    val_label = val_label.cuda()\n",
    "                    val_score = net(val_feature)\n",
    "                    val_loss = loss_function(val_score, val_label)\n",
    "                    val_acc += accuracy_score(torch.argmax(val_score.cpu().data, dim=1), val_label.cpu())\n",
    "                    val_losses += val_loss\n",
    "            end = time.time()\n",
    "            runtime = end - start\n",
    "            pbar.set_postfix({'epoch': '%d' % (epoch),\n",
    "                              'train loss': '%.4f' % (train_loss.data / n),\n",
    "                              'train acc': '%.2f' % (train_acc / n),\n",
    "                              'val loss': '%.4f' % (val_losses.data / m),\n",
    "                              'val acc': '%.2f' % (val_acc / m),\n",
    "                              'time': '%.2f' % (runtime)\n",
    "                              })\n",
    "\n",
    "            # tqdm.write('{epoch: %d, train loss: %.4f, train acc: %.2f, val loss: %.4f, val acc: %.2f, time: %.2f}' %\n",
    "            #       (epoch, train_loss.data / n, train_acc / n, val_losses.data / m, val_acc / m, runtime))\n",
    "\n",
    "    test_pred = []\n",
    "    with torch.no_grad():\n",
    "        with tqdm(total=len(test_iter), desc='Prediction') as pbar:\n",
    "            for test_feature, in test_iter:\n",
    "                test_feature = test_feature.cuda()\n",
    "                test_score = net(test_feature)\n",
    "                # test_pred.extent\n",
    "                test_pred.extend(torch.argmax(test_score.cpu().data, dim=1).numpy().tolist())\n",
    "\n",
    "                pbar.update(1)\n",
    "\n",
    "    result_output = pd.DataFrame(data={\"id\": test[\"id\"], \"sentiment\": test_pred})\n",
    "    result_output.to_csv(\"/kaggle/working/attention_lstm.csv\", index=False, quoting=3)\n",
    "    logging.info('result saved!')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4aba8fef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T15:44:46.872828Z",
     "iopub.status.busy": "2025-10-24T15:44:46.872555Z",
     "iopub.status.idle": "2025-10-24T15:50:07.168750Z",
     "shell.execute_reply": "2025-10-24T15:50:07.168048Z"
    },
    "papermill": {
     "duration": 321.149218,
     "end_time": "2025-10-24T15:50:07.170006",
     "exception": false,
     "start_time": "2025-10-24T15:44:46.020788",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 313/313 [00:30<00:00, 10.18it/s, epoch=0, train loss=0.5639, train acc=0.71, val loss=0.4407, val acc=0.84, time=30.75]\n",
      "Epoch 1: 100%|██████████| 313/313 [00:30<00:00, 10.16it/s, epoch=1, train loss=0.4323, train acc=0.83, val loss=0.3936, val acc=0.86, time=30.82]\n",
      "Epoch 2: 100%|██████████| 313/313 [00:30<00:00, 10.20it/s, epoch=2, train loss=0.3951, train acc=0.85, val loss=0.3812, val acc=0.85, time=30.69]\n",
      "Epoch 3: 100%|██████████| 313/313 [00:30<00:00, 10.21it/s, epoch=3, train loss=0.3920, train acc=0.84, val loss=0.4258, val acc=0.81, time=30.67]\n",
      "Epoch 4: 100%|██████████| 313/313 [00:30<00:00, 10.17it/s, epoch=4, train loss=0.3544, train acc=0.86, val loss=0.3313, val acc=0.88, time=30.80]\n",
      "Epoch 5: 100%|██████████| 313/313 [00:30<00:00, 10.16it/s, epoch=5, train loss=0.3337, train acc=0.87, val loss=0.3174, val acc=0.87, time=30.81]\n",
      "Epoch 6: 100%|██████████| 313/313 [00:30<00:00, 10.19it/s, epoch=6, train loss=0.3212, train acc=0.88, val loss=0.3161, val acc=0.88, time=30.72]\n",
      "Epoch 7: 100%|██████████| 313/313 [00:30<00:00, 10.22it/s, epoch=7, train loss=0.3133, train acc=0.88, val loss=0.3780, val acc=0.85, time=30.64]\n",
      "Epoch 8: 100%|██████████| 313/313 [00:30<00:00, 10.17it/s, epoch=8, train loss=0.3036, train acc=0.88, val loss=0.2968, val acc=0.88, time=30.79]\n",
      "Epoch 9: 100%|██████████| 313/313 [00:30<00:00, 10.18it/s, epoch=9, train loss=0.2905, train acc=0.89, val loss=0.3621, val acc=0.84, time=30.77]\n",
      "Prediction: 100%|██████████| 391/391 [00:11<00:00, 33.17it/s]\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "test = pd.read_csv(\"/kaggle/input/corpus-imdb/testData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\n",
    "\n",
    "num_epochs = 10\n",
    "embed_size = 300\n",
    "num_hiddens = 128\n",
    "num_layers = 2\n",
    "bidirectional = True\n",
    "batch_size = 64\n",
    "labels = 2\n",
    "lr = 0.0001\n",
    "device = torch.device('cuda:0')\n",
    "use_gpu = True\n",
    "\n",
    "\n",
    "class Capsule(nn.Module):\n",
    "    def __init__(self, num_hiddens, bidirectional, num_capsule=5, dim_capsule=5, routings=4, **kwargs):\n",
    "        super(Capsule, self).__init__(**kwargs)\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.bidirectional = bidirectional\n",
    "\n",
    "        self.num_capsule = num_capsule\n",
    "        self.dim_capsule = dim_capsule\n",
    "        self.routings = routings\n",
    "        self.activation = self.squash\n",
    "\n",
    "        # if self.bidirectional:\n",
    "        #     self.W = nn.Parameter(\n",
    "        #         nn.init.xavier_normal_(torch.empty(1, self.num_hiddens * 2, self.num_capsule * self.num_hiddens * 2)))\n",
    "        # else:\n",
    "        #     self.W = nn.Parameter(\n",
    "        #         nn.init.xavier_normal_(torch.empty(1, self.num_hiddens, self.num_capsule * self.num_hiddens)))\n",
    "\n",
    "        input_dim = num_hiddens * 2 if bidirectional else num_hiddens\n",
    "        output_dim = num_capsule * dim_capsule\n",
    "\n",
    "        self.W = nn.Parameter(\n",
    "            nn.init.xavier_normal_(torch.empty(1, input_dim, output_dim))\n",
    "        )\n",
    "        \n",
    "    # def forward(self, inputs):\n",
    "    #     # print(inputs.shape)\n",
    "    #     # print(self.W.shape)\n",
    "    #     u_hat_vecs = torch.matmul(inputs, self.W)\n",
    "    #     batch_size = inputs.size(0)\n",
    "    #     input_num_capsule = inputs.size(1)\n",
    "    #     print(u_hat_vecs.shape)\n",
    "    #     u_hat_vecs = u_hat_vecs.view((batch_size, input_num_capsule,\n",
    "    #                                   self.num_capsule, self.dim_capsule))\n",
    "\n",
    "    #     u_hat_vecs = u_hat_vecs.permute(0, 2, 1, 3).contiguous()  # (batch_size,num_capsule,input_num_capsule,dim_capsule)\n",
    "    #     with torch.no_grad():\n",
    "    #         b = torch.zeros_like(u_hat_vecs[:, :, :, 0])\n",
    "    #     for i in range(self.routings):\n",
    "    #         c = torch.nn.functional.softmax(b, dim=1)  # (batch_size,num_capsule,input_num_capsule)\n",
    "    #         outputs = self.activation(torch.sum(c.unsqueeze(-1) * u_hat_vecs, dim=2))  # bij,bijk->bik\n",
    "    #         if i < self.routings - 1:\n",
    "    #             b = (torch.sum(outputs.unsqueeze(2) * u_hat_vecs, dim=-1))  # bik,bijk->bij\n",
    "        \n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        batch_size = inputs.size(0)\n",
    "        seq_len = inputs.size(1)\n",
    "        # (B, L, input_dim) @ (1, input_dim, output_dim) -> (B, L, output_dim)\n",
    "        u_hat_vecs = torch.matmul(inputs, self.W)  # (B, L, num_capsule * dim_capsule)\n",
    "        # reshape 到 (B, L, num_capsule, dim_capsule)\n",
    "        u_hat_vecs = u_hat_vecs.view(batch_size, seq_len, self.num_capsule, self.dim_capsule)\n",
    "        # 转置为 (B, num_capsule, L, dim_capsule) 便于 routing\n",
    "        u_hat_vecs = u_hat_vecs.permute(0, 2, 1, 3).contiguous()\n",
    "        # 初始化 logits b\n",
    "        b = torch.zeros(batch_size, self.num_capsule, seq_len, device=inputs.device)\n",
    "\n",
    "        for i in range(self.routings):\n",
    "            # softmax over input capsules (dim=1)\n",
    "            c = F.softmax(b, dim=1)  # (B, num_capsule, L)\n",
    "            # weighted sum: (B, num_capsule, L, 1) * (B, num_capsule, L, dim) -> (B, num_capsule, dim)\n",
    "            outputs = torch.sum(c.unsqueeze(-1) * u_hat_vecs, dim=2)  # (B, num_capsule, dim_capsule)\n",
    "            outputs = self.squash(outputs)\n",
    "            if i < self.routings - 1:\n",
    "                # update b: agreement between output and predictions\n",
    "                # (B, num_capsule, 1, dim) * (B, num_capsule, L, dim) -> (B, num_capsule, L)\n",
    "                b = torch.sum(outputs.unsqueeze(2) * u_hat_vecs, dim=-1)    \n",
    "        return outputs  # (batch_size, num_capsule, dim_capsule)\n",
    "\n",
    "    @staticmethod\n",
    "    def squash(x, axis=-1):\n",
    "        s_squared_norm = (x ** 2).sum(axis, keepdim=True)\n",
    "        scale = torch.sqrt(s_squared_norm + 1e-7)\n",
    "        return x / scale\n",
    "\n",
    "\n",
    "class SentimentNet(nn.Module):\n",
    "    def __init__(self, embed_size, num_hiddens, num_layers, bidirectional, weight, labels, use_gpu, **kwargs):\n",
    "        super(SentimentNet, self).__init__(**kwargs)\n",
    "        self.embed_size = embed_size\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.num_layers = num_layers\n",
    "        self.use_gpu = use_gpu\n",
    "        self.bidirectional = bidirectional\n",
    "        self.embedding = nn.Embedding.from_pretrained(weight)\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        self.encoder = nn.LSTM(input_size=self.embed_size, hidden_size=self.num_hiddens,\n",
    "                               num_layers=self.num_layers, bidirectional=self.bidirectional,\n",
    "                               dropout=0)\n",
    "        # self.attention = Attention(num_hiddens=self.num_hiddens, bidirectional=self.bidirectional)\n",
    "        self.capsule = Capsule(num_hiddens=self.num_hiddens, bidirectional=self.bidirectional)\n",
    "        # if self.bidirectional:\n",
    "        #     self.decoder = nn.Linear(num_hiddens * 4, labels)\n",
    "        # else:\n",
    "        #     self.decoder = nn.Linear(num_hiddens * 2, labels)\n",
    "        self.decoder = nn.Linear(self.capsule.dim_capsule * 2, labels)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeddings = self.embedding(inputs)\n",
    "        states, hidden = self.encoder(embeddings.permute(1, 0, 2))\n",
    "        # print(states.shape)\n",
    "        states = states.permute(1, 0, 2)\n",
    "        capsule = self.capsule(states)\n",
    "        encoding = torch.cat([capsule[:, 0], capsule[:, -1]], dim=1)\n",
    "        outputs = self.decoder(encoding)\n",
    "        # print(outputs)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    program = os.path.basename(sys.argv[0])\n",
    "    logger = logging.getLogger(program)\n",
    "\n",
    "    logging.basicConfig(format='%(asctime)s: %(levelname)s: %(message)s')\n",
    "    logging.root.setLevel(level=logging.INFO)\n",
    "    logger.info(r\"running %s\" % ''.join(sys.argv))\n",
    "\n",
    "    logging.info('loading data...')\n",
    "    pickle_file = os.path.join('/kaggle/input/pickle', 'imdb_glove.pickle3')\n",
    "    [train_features, train_labels, val_features, val_labels, test_features, weight, word_to_idx, idx_to_word,\n",
    "     vocab] = pickle.load(open(pickle_file, 'rb'))\n",
    "    logging.info('data loaded!')\n",
    "\n",
    "    net = SentimentNet(embed_size=embed_size, num_hiddens=num_hiddens, num_layers=num_layers,\n",
    "                       bidirectional=bidirectional, weight=weight,\n",
    "                       labels=labels, use_gpu=use_gpu)\n",
    "    net.to(device)\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "    train_set = torch.utils.data.TensorDataset(train_features, train_labels)\n",
    "    val_set = torch.utils.data.TensorDataset(val_features, val_labels)\n",
    "    test_set = torch.utils.data.TensorDataset(test_features)\n",
    "\n",
    "    train_iter = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "    val_iter = torch.utils.data.DataLoader(val_set, batch_size=batch_size, shuffle=False)\n",
    "    test_iter = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        start = time.time()\n",
    "        train_loss, val_losses = 0, 0\n",
    "        train_acc, val_acc = 0, 0\n",
    "        n, m = 0, 0\n",
    "        with tqdm(total=len(train_iter), desc='Epoch %d' % epoch) as pbar:\n",
    "            for feature, label in train_iter:\n",
    "                n += 1\n",
    "                net.zero_grad()\n",
    "                feature = Variable(feature.cuda())\n",
    "                label = Variable(label.cuda())\n",
    "                score = net(feature)\n",
    "                loss = loss_function(score, label)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_acc += accuracy_score(torch.argmax(score.cpu().data,\n",
    "                                                         dim=1), label.cpu())\n",
    "                train_loss += loss\n",
    "\n",
    "                pbar.set_postfix({'epoch': '%d' % (epoch),\n",
    "                                  'train loss': '%.4f' % (train_loss.data / n),\n",
    "                                  'train acc': '%.2f' % (train_acc / n)\n",
    "                                  })\n",
    "                pbar.update(1)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for val_feature, val_label in val_iter:\n",
    "                    m += 1\n",
    "                    val_feature = val_feature.cuda()\n",
    "                    val_label = val_label.cuda()\n",
    "                    val_score = net(val_feature)\n",
    "                    val_loss = loss_function(val_score, val_label)\n",
    "                    val_acc += accuracy_score(torch.argmax(val_score.cpu().data, dim=1), val_label.cpu())\n",
    "                    val_losses += val_loss\n",
    "            end = time.time()\n",
    "            runtime = end - start\n",
    "            pbar.set_postfix({'epoch': '%d' % (epoch),\n",
    "                              'train loss': '%.4f' % (train_loss.data / n),\n",
    "                              'train acc': '%.2f' % (train_acc / n),\n",
    "                              'val loss': '%.4f' % (val_losses.data / m),\n",
    "                              'val acc': '%.2f' % (val_acc / m),\n",
    "                              'time': '%.2f' % (runtime)\n",
    "                              })\n",
    "\n",
    "            # tqdm.write('{epoch: %d, train loss: %.4f, train acc: %.2f, val loss: %.4f, val acc: %.2f, time: %.2f}' %\n",
    "            #       (epoch, train_loss.data / n, train_acc / n, val_losses.data / m, val_acc / m, runtime))\n",
    "\n",
    "    test_pred = []\n",
    "    with torch.no_grad():\n",
    "        with tqdm(total=len(test_iter), desc='Prediction') as pbar:\n",
    "            for test_feature, in test_iter:\n",
    "                test_feature = test_feature.cuda()\n",
    "                test_score = net(test_feature)\n",
    "                # test_pred.extent\n",
    "                test_pred.extend(torch.argmax(test_score.cpu().data, dim=1).numpy().tolist())\n",
    "\n",
    "                pbar.update(1)\n",
    "\n",
    "    result_output = pd.DataFrame(data={\"id\": test[\"id\"], \"sentiment\": test_pred})\n",
    "    result_output.to_csv(\"/kaggle/working/capsule_lstm.csv\", index=False, quoting=3)\n",
    "    logging.info('result saved!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ab6a4f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T15:50:09.393972Z",
     "iopub.status.busy": "2025-10-24T15:50:09.393652Z",
     "iopub.status.idle": "2025-10-24T15:51:05.309833Z",
     "shell.execute_reply": "2025-10-24T15:51:05.308923Z"
    },
    "papermill": {
     "duration": 57.002907,
     "end_time": "2025-10-24T15:51:05.311100",
     "exception": false,
     "start_time": "2025-10-24T15:50:08.308193",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 313/313 [00:05<00:00, 59.35it/s, epoch=0, train loss=0.6874, train acc=0.54, val loss=0.7671, val acc=0.49, time=5.29]\n",
      "Epoch 1: 100%|██████████| 313/313 [00:05<00:00, 59.98it/s, epoch=1, train loss=0.4800, train acc=0.77, val loss=0.3604, val acc=0.85, time=5.22]\n",
      "Epoch 2: 100%|██████████| 313/313 [00:05<00:00, 59.79it/s, epoch=2, train loss=0.3676, train acc=0.84, val loss=0.3419, val acc=0.86, time=5.24]\n",
      "Epoch 3: 100%|██████████| 313/313 [00:05<00:00, 59.63it/s, epoch=3, train loss=0.3371, train acc=0.85, val loss=0.3690, val acc=0.84, time=5.26]\n",
      "Epoch 4: 100%|██████████| 313/313 [00:05<00:00, 59.48it/s, epoch=4, train loss=0.3128, train acc=0.87, val loss=0.3189, val acc=0.87, time=5.27]\n",
      "Epoch 5: 100%|██████████| 313/313 [00:05<00:00, 59.22it/s, epoch=5, train loss=0.2981, train acc=0.87, val loss=0.3258, val acc=0.87, time=5.29]\n",
      "Epoch 6: 100%|██████████| 313/313 [00:05<00:00, 59.31it/s, epoch=6, train loss=0.2712, train acc=0.88, val loss=0.4473, val acc=0.82, time=5.28]\n",
      "Epoch 7: 100%|██████████| 313/313 [00:05<00:00, 59.21it/s, epoch=7, train loss=0.2426, train acc=0.90, val loss=0.3518, val acc=0.86, time=5.29]\n",
      "Epoch 8: 100%|██████████| 313/313 [00:05<00:00, 59.73it/s, epoch=8, train loss=0.2162, train acc=0.91, val loss=0.4819, val acc=0.81, time=5.25]\n",
      "Epoch 9: 100%|██████████| 313/313 [00:05<00:00, 59.55it/s, epoch=9, train loss=0.1912, train acc=0.92, val loss=0.6486, val acc=0.78, time=5.26]\n",
      "Prediction: 100%|██████████| 391/391 [00:02<00:00, 178.28it/s]\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "test = pd.read_csv(\"/kaggle/input/corpus-imdb/testData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\n",
    "\n",
    "num_epochs = 10\n",
    "max_len = 512\n",
    "\n",
    "embed_size = 300\n",
    "num_filter = 128\n",
    "filter_size = 3\n",
    "pooling_size = 2\n",
    "\n",
    "num_hiddens = 64\n",
    "num_layers = 2\n",
    "\n",
    "bidirectional = True\n",
    "batch_size = 64\n",
    "labels = 2\n",
    "lr = 0.8\n",
    "device = torch.device('cuda:0')\n",
    "use_gpu = True\n",
    "\n",
    "\n",
    "class SentimentNet(nn.Module):\n",
    "    def __init__(self, embed_size, num_filter, filter_size, num_hiddens, num_layers, bidirectional, weight, labels, use_gpu, **kwargs):\n",
    "        super(SentimentNet, self).__init__(**kwargs)\n",
    "        self.embed_size = embed_size\n",
    "        self.num_filter = num_filter\n",
    "        self.filter_size = filter_size\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.labels = labels\n",
    "\n",
    "        self.use_gpu = use_gpu\n",
    "\n",
    "        self.embedding = nn.Embedding.from_pretrained(weight)\n",
    "        self.embedding.weight.requires_grad = False\n",
    "\n",
    "        self.conv1d = nn.Conv1d(self.embed_size, self.num_filter, self.filter_size, padding=1)\n",
    "        self.activate = F.relu\n",
    "\n",
    "        self.encoder = nn.LSTM(input_size=max_len//pooling_size, hidden_size=self.num_hiddens,\n",
    "                              num_layers=self.num_layers, bidirectional=self.bidirectional,\n",
    "                              dropout=0)\n",
    "\n",
    "        if self.bidirectional:\n",
    "            self.decoder = nn.Linear(num_hiddens * 4, labels)\n",
    "        else:\n",
    "            self.decoder = nn.Linear(num_hiddens * 2, labels)\n",
    "\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeddings = self.embedding(inputs)\n",
    "\n",
    "        # cnn\n",
    "        convolution = self.activate(self.conv1d(embeddings.permute([0, 2, 1])))\n",
    "        pooling = F.max_pool1d(convolution, kernel_size=pooling_size)\n",
    "\n",
    "        # lstm (seq_len, batch_size, hidden_dim)\n",
    "        states, hidden = self.encoder(pooling.permute([1, 0, 2]))\n",
    "        encoding = torch.cat([states[0], states[-1]], dim=1)\n",
    "\n",
    "        outputs = self.decoder(encoding)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    program = os.path.basename(sys.argv[0])\n",
    "    logger = logging.getLogger(program)\n",
    "\n",
    "    logging.basicConfig(format='%(asctime)s: %(levelname)s: %(message)s')\n",
    "    logging.root.setLevel(level=logging.INFO)\n",
    "    logger.info(r\"running %s\" % ''.join(sys.argv))\n",
    "\n",
    "    logging.info('loading data...')\n",
    "    pickle_file = os.path.join('/kaggle/input/pickle', 'imdb_glove.pickle3')\n",
    "    [train_features, train_labels, val_features, val_labels, test_features, weight, word_to_idx, idx_to_word,\n",
    "            vocab] = pickle.load(open(pickle_file, 'rb'))\n",
    "    logging.info('data loaded!')\n",
    "\n",
    "    net = SentimentNet(embed_size=embed_size, num_filter=num_filter, filter_size=filter_size,\n",
    "                       num_hiddens=num_hiddens, num_layers=num_layers, bidirectional=bidirectional,\n",
    "                       weight=weight, labels=labels, use_gpu=use_gpu)\n",
    "    net.to(device)\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=lr)\n",
    "\n",
    "    train_set = torch.utils.data.TensorDataset(train_features, train_labels)\n",
    "    val_set = torch.utils.data.TensorDataset(val_features, val_labels)\n",
    "    test_set = torch.utils.data.TensorDataset(test_features, )\n",
    "\n",
    "    train_iter = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "    val_iter = torch.utils.data.DataLoader(val_set, batch_size=batch_size, shuffle=False)\n",
    "    test_iter = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        start = time.time()\n",
    "        train_loss, val_losses = 0, 0\n",
    "        train_acc, val_acc = 0, 0\n",
    "        n, m = 0, 0\n",
    "        with tqdm(total=len(train_iter), desc='Epoch %d' % epoch) as pbar:\n",
    "            for feature, label in train_iter:\n",
    "                n += 1\n",
    "                net.zero_grad()\n",
    "                feature = Variable(feature.cuda())\n",
    "                label = Variable(label.cuda())\n",
    "                score = net(feature)\n",
    "                loss = loss_function(score, label)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_acc += accuracy_score(torch.argmax(score.cpu().data,\n",
    "                                                         dim=1), label.cpu())\n",
    "                train_loss += loss\n",
    "\n",
    "                pbar.set_postfix({'epoch': '%d' % (epoch),\n",
    "                                  'train loss': '%.4f' % (train_loss.data / n),\n",
    "                                  'train acc': '%.2f' % (train_acc / n)\n",
    "                                  })\n",
    "                pbar.update(1)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for val_feature, val_label in val_iter:\n",
    "                    m += 1\n",
    "                    val_feature = val_feature.cuda()\n",
    "                    val_label = val_label.cuda()\n",
    "                    val_score = net(val_feature)\n",
    "                    val_loss = loss_function(val_score, val_label)\n",
    "                    val_acc += accuracy_score(torch.argmax(val_score.cpu().data, dim=1), val_label.cpu())\n",
    "                    val_losses += val_loss\n",
    "            end = time.time()\n",
    "            runtime = end - start\n",
    "            pbar.set_postfix({'epoch': '%d' % (epoch),\n",
    "                              'train loss': '%.4f' % (train_loss.data / n),\n",
    "                              'train acc': '%.2f' % (train_acc / n),\n",
    "                              'val loss': '%.4f' % (val_losses.data / m),\n",
    "                              'val acc': '%.2f' % (val_acc / m),\n",
    "                              'time': '%.2f' % (runtime)})\n",
    "\n",
    "            # tqdm.write('{epoch: %d, train loss: %.4f, train acc: %.2f, val loss: %.4f, val acc: %.2f, time: %.2f}' %\n",
    "            #       (epoch, train_loss.data / n, train_acc / n, val_losses.data / m, val_acc / m, runtime))\n",
    "\n",
    "    test_pred = []\n",
    "    with torch.no_grad():\n",
    "        with tqdm(total=len(test_iter), desc='Prediction') as pbar:\n",
    "            for test_feature, in test_iter:\n",
    "                test_feature = test_feature.cuda()\n",
    "                test_score = net(test_feature)\n",
    "                # test_pred.extent\n",
    "                test_pred.extend(torch.argmax(test_score.cpu().data, dim=1).numpy().tolist())\n",
    "\n",
    "                pbar.update(1)\n",
    "\n",
    "    result_output = pd.DataFrame(data={\"id\": test[\"id\"], \"sentiment\": test_pred})\n",
    "    result_output.to_csv(\"/kaggle/working/cnn_lstm.csv\", index=False, quoting=3)\n",
    "    logging.info('result saved!')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8497927,
     "sourceId": 13392075,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8499338,
     "sourceId": 13393956,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1373.885532,
   "end_time": "2025-10-24T15:51:09.216849",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-10-24T15:28:15.331317",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
